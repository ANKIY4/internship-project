{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3cc6a114b34903e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:20.781701Z",
     "start_time": "2025-04-05T13:01:20.772025Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52801e75e4e7dd00",
   "metadata": {},
   "source": [
    "### **Now in this 3rd component's 1st task we will be Building a baseline model without any feature engineering techniques to see how the models perform without any preprocessing, just to get a baseline understanding of how our models perform on raw data. After feature engineering and scaling the data we will again implement the Machine learning algorithms and see how the accuracy spiked up or down.**\n",
    "---\n",
    "- **<b>Note:</b> we will use the dataset from the 2nd component because we have handeld the outliers quite efficiently and also removed missing data from every column and cleaned the data set we will import the data later when we have need for it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bdeb9e58c0d1cdaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:21.608640Z",
     "start_time": "2025-04-05T13:01:21.579302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "# importing the data set.\n",
    "cdf = pd.read_csv('Datasets/diabetes.csv')\n",
    "\n",
    "# splitting the features(x) and dependent variable(y).\n",
    "X = cdf.iloc[:, :-1].values\n",
    "y = cdf.iloc[:, -1].values\n",
    "\n",
    "# splitting into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "#checking the dimension of the data\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f848d7751eadf3d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:22.013139Z",
     "start_time": "2025-04-05T13:01:21.983613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78, 21],\n",
       "       [18, 37]], dtype=int64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Logistic regression model with it's estimated confusion matrix\n",
    "LRM = LogisticRegression(max_iter=500)\n",
    "LRM.fit(X_train, y_train)\n",
    "y_pred_LR = LRM.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2439c1c24b93e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:22.445505Z",
     "start_time": "2025-04-05T13:01:22.429280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 29],\n",
       "       [23, 32]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a K-Nearest Neighbors model with it's estimated confusion matrix\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9dcc48e533b5a9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:22.858096Z",
     "start_time": "2025-04-05T13:01:22.844490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78, 21],\n",
       "       [17, 38]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a Decision tree classifier model with it's estimated confusion matrix\n",
    "dec_tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "dec_tree.fit(X_train, y_train)\n",
    "y_pred_dec_tree = dec_tree.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_dec_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1867af080dbd8e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:23.389312Z",
     "start_time": "2025-04-05T13:01:23.122388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[81, 18],\n",
       "       [20, 35]], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a random forest classifier model with it's estimated confusion matrix\n",
    "rand_forest = RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy')\n",
    "rand_forest.fit(X_train, y_train)\n",
    "y_pred_rand_forest = rand_forest.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred_rand_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a3091e6bf04bdaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:23.464112Z",
     "start_time": "2025-04-05T13:01:23.446702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score of logistic regression is 0.7467532467532467\n",
      "The accuracy score of K-Nearest Neighbors is 0.6623376623376623\n",
      "The accuracy of Decision Tree classifier is 0.7532467532467533\n",
      "The accuracy of Random forest classifier is 0.7532467532467533\n",
      "\n",
      "\n",
      "The f1 score of logistic regression is 0.6548672566371682\n",
      "The f1 score of K-Nearest Neighbors is 0.5517241379310346\n",
      "The f1 of Decision Tree classifier is 0.6666666666666665\n",
      "The f1 of Random forest classifier is 0.6481481481481481\n"
     ]
    }
   ],
   "source": [
    "# we will use two accuracy metric accuracy_score which uses the total true predicted values divided by total total values in the data set and f1 score which uses precision and recall as an accuracy metric.\n",
    "\n",
    "print(f\"The accuracy score of logistic regression is {accuracy_score(y_test, y_pred_LR)}\")\n",
    "print(f\"The accuracy score of K-Nearest Neighbors is {accuracy_score(y_test, y_pred_knn)}\")\n",
    "print(f\"The accuracy of Decision Tree classifier is {accuracy_score(y_test, y_pred_dec_tree)}\")\n",
    "print(f\"The accuracy of Random forest classifier is {accuracy_score(y_test, y_pred_rand_forest)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The f1 score of logistic regression is {f1_score(y_test, y_pred_LR)}\")\n",
    "print(f\"The f1 score of K-Nearest Neighbors is {f1_score(y_test, y_pred_knn)}\")\n",
    "print(f\"The f1 of Decision Tree classifier is {f1_score(y_test, y_pred_dec_tree)}\")\n",
    "print(f\"The f1 of Random forest classifier is {f1_score(y_test, y_pred_rand_forest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfa57fc79b99768",
   "metadata": {},
   "source": [
    "---\n",
    "### Observations and Next Steps\n",
    "***The accuracy score and F1-score of the model are currently not satisfactory, indicating that the model is performing poorly in predicting the data. To address this, we will now apply feature engineering techniques to enhance the input data and improve model performance. Additionally, to achieve optimal results, we will implement hyperparameter tuning using methods such as GridSearchCV or Optuna, which will help identify the best hyperparameters and further boost the model's effectiveness.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec8056355fbd7c",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242a8f4377ca08e",
   "metadata": {},
   "source": [
    "**let's scale our data we will use Standard scaler but before that we will make a function that trains and print the accuracy for us because we will train our models several times and each time writing a long code is not productive so we will create a function so we can re-use it.**\n",
    "\n",
    "- **In this user defined function we just need to pass the training and testing data and this function will run the models which is initialised in the models dictionary and return us a dictionary with accuracy metrics i.e (accuracy score and f1 score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ffde1525171f1cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:23.922844Z",
     "start_time": "2025-04-05T13:01:23.912630Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_evaluate_classifiers(X_train, X_test, y_train, y_test):\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2),\n",
    "        'Decision Tree': DecisionTreeClassifier(criterion='entropy', random_state=0),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy')\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train) # train the model\n",
    "        y_pred = model.predict(X_test) # predict the outcome based on the test data\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred) # calculating accuracy score\n",
    "        f1 = f1_score(y_test, y_pred) # calculating the f1_score\n",
    "\n",
    "\n",
    "        print(f\"{name}:\") # name of the model\n",
    "        print(f\"  Accuracy Score: {round(acc, 3)}\") # Accuracy Score rounded to 33 decimal places\n",
    "        print(f\"  F1 Score      : {round(f1, 3)}\") # f1 score in rounded to 3 decimal places\n",
    "        print(\"-\" * 40) # a line to separate the scores of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7121e42ec511",
   "metadata": {},
   "source": [
    "### let's start with scaling the data with StandardScaler which converts the data into standard normal distribution with mean 0 and std of 1. this scales the data approximately in the range (-3 - 3) and test the accuracy with the data\n",
    "---\n",
    "- ***Note: we haven't cleaned the dataset we are using the unclean dataset to check how the model will perform by just scaling it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c377812f6a013617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:24.322504Z",
     "start_time": "2025-04-05T13:01:24.022115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Accuracy Score: 0.753\n",
      "  F1 Score      : 0.661\n",
      "----------------------------------------\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy Score: 0.695\n",
      "  F1 Score      : 0.544\n",
      "----------------------------------------\n",
      "Decision Tree:\n",
      "  Accuracy Score: 0.76\n",
      "  F1 Score      : 0.678\n",
      "----------------------------------------\n",
      "Random Forest:\n",
      "  Accuracy Score: 0.753\n",
      "  F1 Score      : 0.648\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "Xscaled_train = sc.fit_transform(X_train)\n",
    "Xscaled_test = sc.transform(X_test)\n",
    "\n",
    "build_and_evaluate_classifiers(Xscaled_train, Xscaled_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d312e0f212a4aaa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:24.618380Z",
     "start_time": "2025-04-05T13:01:24.333167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Accuracy Score: 0.753\n",
      "  F1 Score      : 0.661\n",
      "----------------------------------------\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy Score: 0.695\n",
      "  F1 Score      : 0.544\n",
      "----------------------------------------\n",
      "Decision Tree:\n",
      "  Accuracy Score: 0.76\n",
      "  F1 Score      : 0.678\n",
      "----------------------------------------\n",
      "Random Forest:\n",
      "  Accuracy Score: 0.753\n",
      "  F1 Score      : 0.648\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ms = MinMaxScaler()\n",
    "x_train_ms = ms.fit_transform(X_train)\n",
    "x_test_ms = ms.transform(X_test)\n",
    "\n",
    "build_and_evaluate_classifiers(Xscaled_train, Xscaled_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7123dcd36c313",
   "metadata": {},
   "source": [
    "# **as we can see that the model performed not that good comparing to the baseline model, even through we are scaling the data. Why this happend??**\n",
    "- **there are lots of outliers in the dataset and missing values which is making our model performance weak so now we will use the clean dataset from our component 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a10b992b6c91c00e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:24.821037Z",
     "start_time": "2025-04-05T13:01:24.809503Z"
    }
   },
   "outputs": [],
   "source": [
    "# here we will use the clean data from the component 2\n",
    "cdf = pd.read_csv('Datasets/clean_df_from_component2.csv') # clean dataframe\n",
    "x = cdf.iloc[:, :-1].values\n",
    "y =cdf.iloc[:, -1].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d94cd9f0ea36d368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:25.138459Z",
     "start_time": "2025-04-05T13:01:24.870497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "  Accuracy Score: 0.778\n",
      "  F1 Score      : 0.638\n",
      "----------------------------------------\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy Score: 0.752\n",
      "  F1 Score      : 0.596\n",
      "----------------------------------------\n",
      "Decision Tree:\n",
      "  Accuracy Score: 0.771\n",
      "  F1 Score      : 0.673\n",
      "----------------------------------------\n",
      "Random Forest:\n",
      "  Accuracy Score: 0.752\n",
      "  F1 Score      : 0.604\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "xscaledtrain = sc.fit_transform(x_train)\n",
    "xscaledtest = sc.transform(x_test)\n",
    "\n",
    "build_and_evaluate_classifiers(xscaledtrain, xscaledtest, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01bc8f1c2558c4",
   "metadata": {},
   "source": [
    "**hmmm, now we can see that our accuracy are increased a little and the model performed well we can see the accuracy score and the f1 score(which use precision and recall) that they have been increased comparing to the baseline model we built earlier and the model that we used scaling in unclean dataset. this shows how much is it importent to feature engineer our dataset.**\n",
    "\n",
    "---\n",
    "- **Now we will see which features(columns) are the most important one in the dataset using feature importance technique with random RandomForestClassifier. it gives us the ranking of each feature with certain values the grater the value the more it has importance to the dependent variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9aba1577dd7bee9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:25.404642Z",
     "start_time": "2025-04-05T13:01:25.138459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Importance\n",
      "1                   Glucose    0.281294\n",
      "4                       BMI    0.176908\n",
      "6                       Age    0.141002\n",
      "5  DiabetesPedigreeFunction    0.130520\n",
      "2             BloodPressure    0.094809\n",
      "0               Pregnancies    0.091749\n",
      "3             SkinThickness    0.083717\n"
     ]
    }
   ],
   "source": [
    "# feature importance using random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define features and target\n",
    "X = cdf.drop(columns=['Outcome'])  # Drop target variable\n",
    "y = cdf['Outcome']\n",
    "\n",
    "# train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# get the feature importances of every column\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Print the values\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e759e0a1e5243",
   "metadata": {},
   "source": [
    "**so the columns Glucose is the most importance and the highest one followed by BMI, Age, DiabetesPedigreeFunction has the highest importance as they have the highest values in the result.so, these are the top feature in the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88ce1337fc9041",
   "metadata": {},
   "source": [
    "---\n",
    "**now we have seen that applying feature engineeing and scaling the data is very much important as we can see from the above results the accuracy and the f1 score of each model significantly increased when we used the clean data and scaled it compared to the dirty data with scaling without any preprocessing other than scaling now we will apply GridSearchCV to tune our model and enhance the performance in other words we wil do hyperparameter tuning in each model and see which gives us the best accuracy**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5677b328dedd7fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T13:01:28.643388Z",
     "start_time": "2025-04-05T13:01:25.435374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Best Estimator: LogisticRegression(C=0.1, max_iter=500, solver='liblinear')\n",
      "Best Cross-Validation Score: 0.7671891124871001\n",
      "Best Hyperparameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test Accuracy: 0.7908496732026143\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KNN:\n",
      "Best Estimator: KNeighborsClassifier(n_neighbors=7, p=1)\n",
      "Best Cross-Validation Score: 0.7212439800481596\n",
      "Best Hyperparameters: {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
      "Test Accuracy: 0.7777777777777778\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decision Tree:\n",
      "Best Estimator: DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=5)\n",
      "Best Cross-Validation Score: 0.704968610251118\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}\n",
      "Test Accuracy: 0.7647058823529411\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Random Forest:\n",
      "Best Estimator: RandomForestClassifier(n_estimators=200)\n",
      "Best Cross-Validation Score: 0.7655873753009976\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test Accuracy: 0.7581699346405228\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# took all the hyperparameters from the sklearn docs. here in this models dictionary we create a key value paris of model name(key) and value(a tuple with model initialization and parameter grids) which the grid search will use exhaustively to search the optimal hyperparameters. we will use all of this information to implement hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=500), {\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear']\n",
    "    }),\n",
    "    'KNN': (KNeighborsClassifier(), {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    }),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }),\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5]\n",
    "    })\n",
    "}\n",
    "\n",
    "best_models = {} # This dictionary will store the best results for each model.\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=4, scoring='accuracy', n_jobs=-1) # initialized GridSearchCV for Hyperparameter Tuning\n",
    "\n",
    "    grid_search.fit(xscaledtrain, y_train) # Fit the GridSearchCV to the training data (X_train, y_train) and it will perform the cross validation upto 4 folds with each combination of hyperparameters\n",
    "\n",
    "    best_models[name] = {\n",
    "        \"estimator\": grid_search.best_estimator_, # the best estimator meaning the model with best hyperparameters\n",
    "        \"Best Score\": grid_search.best_score_, # best cross validation score.\n",
    "        \"best_params\": grid_search.best_params_, # best hyperparameters(the one we need the most)\n",
    "        \"test_score\": grid_search.best_estimator_.score(xscaledtest, y_test) # test score with the hyperparameters\n",
    "    }\n",
    "\n",
    "# this just loops in the best model dictionary and prints each key value pair results including the best estimator, optimal hyperparameters, accuracy etc.\n",
    "for model_name, result in best_models.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"Best Estimator: {result['estimator']}\")\n",
    "    print(f\"Best Cross-Validation Score: {result['Best Score']}\")\n",
    "    print(f\"Best Hyperparameters: {result['best_params']}\")\n",
    "    print(f\"Test Accuracy: {result['test_score']}\")\n",
    "    print(\"-\" * 100) # just prints the dashed line to separate each iteration for each model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ace1adb118ca9",
   "metadata": {},
   "source": [
    "# Observation\n",
    "**from this code cell above we tried to optimize and tune our model as much as we can we applied cross validation, grid search for finding optimal hyperparameters, we feature engineered the dataset handled outliers, handles missing values scaled the data we saw how feature engineering and scaling the data drastically changes the outcome of the accuracy of the model.**\n",
    "\n",
    "---\n",
    "# The end✨\n",
    "## Author\n",
    "### Ankit chimariya\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
